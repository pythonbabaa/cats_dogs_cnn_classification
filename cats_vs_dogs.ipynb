{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c776d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda, nn, optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# libraries for runbuilder class\n",
    "\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099a5ce",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a0c6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on cpu\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training model on {device}\\n{\"=\" *25}')\n",
    "\n",
    "# The f means Formatted string literals and it's new in Python 3.6.\n",
    "# A formatted string literal or f-string is a string literal that is prefixed with 'f' or 'F'.\n",
    "# These strings may contain replacement fields, which are expressions delimited by curly braces {}.\n",
    "# While other string literals always have a constant value, \n",
    "# formatted strings are really expressions evaluated at run time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423cdc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '/home/a/Documents/cat and dogs data set'\n",
    "train_set = data_directory + '/training_set'\n",
    "test_set = data_directory + '/test_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(test_set)  # os.listdir produces just list of names in directory.\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ea884",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = os.listdir(train_set + \"/catos\")  # os.listdir produces just list of names in directory.\n",
    "cats[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = os.listdir(train_set + \"/dogys\")  # os.listdir produces just list of names in directory.\n",
    "dogs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed18037",
   "metadata": {},
   "source": [
    "The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815b1d4",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeca8fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since the data consists of color images with 3 channels (RGB), each image tensor has the shape of varying size.\n",
    "# So we are resizing the image to have same shape (200,200).\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((200, 200)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageFolder: A generic data loader where the images are arranged in this way: root/dog/xxx.png\n",
    "train_dataset = ImageFolder(train_set, transform = transform)\n",
    "test_dataset = ImageFolder(test_set, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb4f16",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "__Feature Scaling is same as data / dataset normalization.__\n",
    "\n",
    "Feature scaling referes to the fact that when we are normalizing data/datasets, we are often normalizing various features of that dataset to a similar scale.\n",
    "\n",
    "__Standardization (calculating z score) is a specific type of normalization technique.__ We will do standardization of training set only.\n",
    "\n",
    "_torchvision.transforms.Normalize(\n",
    "      [MeanofChannel1, MeanofChannel2, MeanofChannel3] \n",
    "    , [StdofChannel1, StdofChannel2, StdofChannel3] \n",
    ")_\n",
    "\n",
    "Hence we need to calculate mean and standard deviation of each channel. For RGB image, there are 3 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b583dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to manually implement the formulas for the mean and standard deviation and iterate over smaller batches of the dataset. \n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=64, num_workers=1)\n",
    "num_of_pixels = len(train_dataset) * 200 * 200 #Note that the 200 * 200 is the height and width of the images inside our dataset.\n",
    "\n",
    "# Now, we sum the pixels values by iterating over each batch.\n",
    "# and we calculate the mean by dividing this sum by the total number of pixels.\n",
    "\n",
    "\n",
    "def normalize_image_tensor(loader):\n",
    "    total_sum = 0\n",
    "    sum_of_squared_error = 0\n",
    "    for images, label in loader:\n",
    "\n",
    "        if images.shape[1] == 1:\n",
    "            total_sum += images.sum()\n",
    "            mean = total_sum / num_of_pixels\n",
    "            print(mean)\n",
    "            return mean\n",
    "\n",
    "        elif images.shape[1] == 3:\n",
    "            total_sum += images[0].sum()\n",
    "            mean0 = total_sum / num_of_pixels\n",
    "\n",
    "            total_sum += images[1].sum()\n",
    "            mean1 = total_sum / num_of_pixels\n",
    "\n",
    "            total_sum += images[2].sum()\n",
    "            mean2 = total_sum / num_of_pixels\n",
    "            \n",
    "            print(mean0, mean1, mean2)\n",
    "            \n",
    "            \n",
    "            return mean0, mean1, mean2\n",
    "\n",
    "# Next, we calculate the sum of the squared errors by iterating thorough each batch,\n",
    "# and this allows us to calculate the standard deviation\n",
    "# by dividing the sum of the squared errors by the total number of pixels and square rooting the result.\n",
    "\n",
    "\n",
    "#     for images, label in loader:\n",
    "        \n",
    "#         if images.shape[0] == 1:\n",
    "#             sum_of_squared_error += ((images - mean).pow(2)).sum()\n",
    "#             std = torch.sqrt(sum_of_squared_error / num_of_pixels)\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((200, 200)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageFolder: A generic data loader where the images are arranged in this way: root/dog/xxx.png\n",
    "normalize_train_dataset = ImageFolder(train_set, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a24d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalize_train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = normalize_train_dataset[3100]\n",
    "img.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881261a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[500]\n",
    "img.shape, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e1a0f",
   "metadata": {},
   "source": [
    "As an internal functionality, the list of classes is stored in the .classes property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fbad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582812c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a64919",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdec856",
   "metadata": {},
   "source": [
    "We can view the image using matplotlib, but we need to change the tensor dimensions to (200,200,3). Notice, for matplotlib the channel should be specified in the third position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae461c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "# img.permute() returns a view of the original tensor with its dimensions permuted.\n",
    "# Example:\n",
    "#     >>> x = torch.randn(2, 3, 5)\n",
    "#     >>> x.size()\n",
    "#     torch.Size([2, 3, 5])\n",
    "#     >>> x.permute(2, 0, 1).size()\n",
    "#     torch.Size([5, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd443e",
   "metadata": {},
   "source": [
    "### Training and validation datasets\n",
    "\n",
    "_While building real world machine learning models, it is quite common to split the dataset into 3 parts:_\n",
    "\n",
    "1. __Training set__ - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n",
    "\n",
    "\n",
    "2. __Validation set__ - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n",
    "\n",
    "\n",
    "3. __Test set__ - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model. Since there's no predefined validation set, we can set aside a small portion of the training set to be used as the validation set. We'll use the random_split helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ee9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normalize_train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_size = 2000\n",
    "train_set_size = 6005\n",
    "\n",
    "train_ds, val_ds = random_split(normalize_train_dataset, [train_set_size, validation_set_size],\n",
    "                                generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
    "# Optionally fix the generator for reproducible results, e.g.:\n",
    "# random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c7eec",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "We can now create data loaders to help us load the data in batches. Large datasets requires loading them into memory all at once. This leads to memory outage and slowing down of programs. PyTorch offers a solution for parallelizing the data loading process with the support of automatic batching as well. This is the DataLoader class present within the torch.utils.data package.\n",
    "\n",
    "We'll use a batch size of 64, we will load 64 samples at a time until all the 50000 images in the training set are loaded and trained to complete 1 epoch.\n",
    "\n",
    "__What is batch size__?\n",
    "\n",
    "The number of samples (data points) that would be passed through the network at a time.\n",
    "\n",
    "__What is epoch?__\n",
    "\n",
    "An epoch is one single pass of all the input data through the network.\n",
    "\n",
    "__Relation between batch_size and epoch?__\n",
    "\n",
    "batch_size is not equal to epoch, consider you have 1000 images. Processing all the 1000 images through the network once is considered as 1 epoch. If we set the batch size as 10, during training we will be passing 100 data points (=1000/10) at a time until we eventually pass in all the training data to complete 1 single epoch.\n",
    "\n",
    "_Generally, larger the batch size faster the training. However, you need to have enough hardware to handle. Sometimes, even if our machine can handle heavy computation,by setting larger batch size quality of the model could degrade and could create difficulty in generalizing._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee00072",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = True, num_workers = 1, pin_memory = True)\n",
    "val_loader = DataLoader(val_ds, batch_size = batch_size, num_workers = 1, pin_memory = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, num_workers = 1, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b2bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "data[0].mean(), data[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainimages, trainlabels = next(iter(train_loader))\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "print('length of training set: ', len(train_ds))\n",
    "print(data[0].shape) # shape of image\n",
    "print(data[0].mean(), data[0].std()) # mean and standard deviation of image tensor\n",
    "print('training images')\n",
    "\n",
    "plt.imshow(trainimages[0].permute(1,2,0))\n",
    "print(trainlabels[0])\n",
    "\n",
    "print(trainimages[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    fig, ax = plt.subplots(figsize = (16, 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(images[0].permute(1, 2, 0))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b57b9",
   "metadata": {},
   "source": [
    "__Print batch images__\n",
    "\n",
    "We can look at batches of images from the dataset using the make_grid method from torchvision. Each time the following code is run, we get a different bach, since the sampler shuffles the indices before creating batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408be73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    fig, ax = plt.subplots(figsize = (16, 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(images, nrow=16, padding = 2).permute(1, 2, 0))\n",
    "    #print(labels[0])\n",
    "    print(images[0].shape)\n",
    "    #print(images[0])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c681f",
   "metadata": {},
   "source": [
    "### Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'training model on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5968f",
   "metadata": {},
   "source": [
    "### Evaluation Metric and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05190125",
   "metadata": {},
   "source": [
    "Let's first define out evaluation metric, we need a way to evaluate how well our model is performing. A natural way to do this would be to find the percentage of labels that were predicted correctly i.e. the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `.item()` function just returns the Python value from the tensor.\n",
    "\n",
    "# def accuracy(outputs, labels):\n",
    "#     _, preds = torch.max(outputs, dim=1)  # dim=0, (maximum along columns), dim=1 (maximum along rows).\n",
    "#     return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def get_num_correct(outputs, labels):\n",
    "    return outputs.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5cd31",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here we are using torch.max() function, this function's default behaviour as you can guess by the name is to return maximum among the elements in the Tensor. However, this function also helps get the maximum along a particular dimension, as a Tensor, instead of a single element. To specify the dimension (axis – in numpy), there is another optional keyword argument, called dim. This represents the direction that we take for the maximum.\n",
    "\n",
    "max_elements, max_indices = torch.max(input_tensor, dim)\n",
    "\n",
    "    dim=0, (maximum along columns).\n",
    "    dim=1 (maximum along rows).\n",
    "\n",
    "This returns a tuple, max_elements and max_indices.\n",
    "\n",
    "    max_elements -> All the maximum elements of the Tensor.\n",
    "    max_indices -> Indices corresponding to the maximum elements.\n",
    "\n",
    "In the above accuracy function, the == performs an element-wise comparison of two tensors with the same shape, and returns a tensor of the same shape, containing 0s for unequal elements, and 1s for equal elements. Passing the result to torch.sum returns the number of labels that were predicted correctly. Finally, we divide by the total number of images to get the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d615ef1",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05f8f4",
   "metadata": {},
   "source": [
    "While the accuracy is a great way for us (humans) to evaluate the model, it can't be used as a loss function for optimizing our model using gradient descent, for the following reasons:\n",
    "\n",
    "It's not a differentiable function. torch.max and == are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n",
    "\n",
    "It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements.\n",
    "\n",
    "Due to these reasons, accuracy is a great evaluation metric for classification, but not a good loss function. A commonly used loss function for classification problems is the cross entropy,\n",
    "How Cross Entropy works\n",
    "\n",
    "For each output row, pick the predicted probability for the correct label. E.g. if the predicted probabilities for an image are [0.1, 0.3, 0.2, ...] and the correct label is 1, we pick the corresponding element 0.3 and ignore the rest.\n",
    "\n",
    "Then, take the logarithm of the picked probability. If the probability is high i.e. close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiply the result by -1, which results is a large postive value of the loss for poor predictions.\n",
    "\n",
    "Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data.\n",
    "\n",
    "Unlike accuracy, cross-entropy is a continuous and differentiable function that also provides good feedback for incremental improvements in the model (a slightly higher probability for the correct label leads to a lower loss). This makes it a good choice for the loss function.\n",
    "\n",
    "__PyTorch provides an efficient and tensor-friendly implementation of cross entropy as part of the torch.nn.functional package. Moreover, it also performs softmax internally, so we can directly pass in the outputs of the model without converting them into probabilities.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044a161",
   "metadata": {},
   "source": [
    "### Making neural network model architecture\n",
    "\n",
    "with __batch normalization__, without __dropout layer__ (nn.Dropout(p=0.5)) and without __gradient clipping__ nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "\n",
    "A __dropout layer__ sets a certain amount of neurons to zero. The argument we passed, p=0.5 is the probability that any neuron is set to zero. So every time we run the code, the sum of nonzero values should be approximately reduced by half. Imagine a 2d matrix of size 5x5 filled with ones. The sum of nonzero values would be 5*5=25. After the dropout, roughly half of the 1 will turn into 0. So the sum of nonzero values would be around 12.\n",
    "\n",
    "__Gradient clipping__ is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. It prevents the gradients of a network from becoming overly large by simply preventing them from exceeding some threshold value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5)\n",
    "        self.batch_norm_2_1 = nn.BatchNorm2d(10)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        self.batch_norm_2_2 = nn.BatchNorm2d(20)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(44180, 200)\n",
    "        self.batch_norm_1_1 = nn.BatchNorm1d(200)\n",
    "        self.fc2 = nn.Linear(200, 2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = self.batch_norm_2_1(F.relu(self.max_pool(self.conv1(x))))   # applying batch normalization\n",
    "        x = self.batch_norm_2_2(F.relu(self.max_pool(self.conv2(x))))\n",
    "        x = x.view(in_size, -1)       # it can also be done through nn.Flatten(dim = 1)\n",
    "        x = self.batch_norm_1_1(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "batch_size = 64\n",
    "model = cnn()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = lr, momentum = momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57313e02",
   "metadata": {},
   "source": [
    "__Summary of neural network architecture__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee7d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdec02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(model, input_size= (3, 200, 200), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f298d8f",
   "metadata": {},
   "source": [
    "### Starting out with TensorBoard (Network Graph and Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec947a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb = SummaryWriter()\n",
    "\n",
    "# model = cnn()\n",
    "# images, labels = next(iter(train_loader))\n",
    "# grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# tb.add_image('images', grid)\n",
    "# tb.add_graph(model, images)\n",
    "# tb.close()\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a3a89",
   "metadata": {},
   "source": [
    "### Training model with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7215372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(model.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f63077",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86375597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# name variable contains both weight and bias terms. \n",
    "# weight contains parameter values.\n",
    "\n",
    "for name, weight in model.named_parameters():\n",
    "    print(f'{name}:   ', weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49556be9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# parameters of gradients\n",
    "\n",
    "# for name, weight in model.named_parameters():\n",
    "#     print(f'{name}.grad:   ', weight.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# will provide cartesian product of all of our parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    lr = [0.01],\n",
    "    batch_size = [100],\n",
    "    shuffle = [True]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_values = [values for values in parameters.values()]\n",
    "param_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr, batch_size, shuffle in product(*param_values):  # will get cartesian product of elements in the list\n",
    "    print(lr, batch_size, shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7d9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lr, batch_size, shuffle in product(*param_values):\n",
    "    comment = f' lr = {lr}, batch_size = {batch_size}, shuffle = {shuffle}'\n",
    " \n",
    "    train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = shuffle, num_workers = 1, pin_memory = True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr = lr, momentum = momentum)\n",
    "\n",
    "    model.train()\n",
    "    print(f'This model is being trained on {device}.')\n",
    "\n",
    "    images, labels = next(iter(train_loader))\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "\n",
    "    #comment = f' batch_size = {batch_size}, lr = {lr}'\n",
    "\n",
    "    # adding above mentioned comment to summary writer will uniquely identify runs inside tensor board because summarywriter will append the comment to the name of the run.\n",
    "\n",
    "    tb = SummaryWriter(comment=comment)\n",
    "    tb.add_image('images', grid)\n",
    "    tb.add_graph(model.to(device), images.to(device))\n",
    "\n",
    "\n",
    "    for epoch in range(2):\n",
    "\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch_no, (images, label) in enumerate(train_loader):\n",
    "            images, label = images.to(device), label.to(device) # .to(device) = .to('cuda') or .to('cpu') whichever is available\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)   # Pass batch\n",
    "            loss = criterion(output, label) # calculate loss\n",
    "            loss.backward()  # calculate gradients\n",
    "            optimizer.step() # update weights\n",
    "\n",
    "            total_loss += loss.item() * batch_size # we are multiplying with batch size to account for different batchsizes  \n",
    "            total_correct += get_num_correct(output, label)\n",
    "\n",
    "            if batch_no % 10 == 0:\n",
    "                print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                        epoch, batch_no * len(images), len(train_loader.dataset),\n",
    "                        100. * batch_no / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "        tb.add_scalar('Loss', total_loss, epoch)\n",
    "        tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "        tb.add_scalar('Accuracy', total_correct/len(train_ds), epoch)\n",
    "\n",
    "    #     tb.add_histogram('conv1.bias', model.conv1.bias, epoch)\n",
    "    #     tb.add_histogram('conv1.weight', model.conv1.weight, epoch)\n",
    "    #     tb.add_histogram('conv1.weight.grad', model.conv1.weight.grad, epoch)\n",
    "\n",
    "#         for name, weight in model.named_parameters():\n",
    "#             tb.add_histogram(tag = name, values = weight, global_step = epoch)\n",
    "#             tb.add_histogram(f'{name}.grad:   ', weight.grad, epoch)\n",
    "\n",
    "        print(\"epoch\", epoch,\n",
    "              \"total_correct\", total_correct,\n",
    "              \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e0cbc",
   "metadata": {},
   "source": [
    "### Training model with Run Builder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class runbuilder():\n",
    "    @staticmethod    #you can call this class directly. No need to make instance\n",
    "    \n",
    "    def get_runs(params):\n",
    "        \n",
    "        Run = namedtuple('Run', params.keys())\n",
    "        \n",
    "        runs = []\n",
    "        for value in product(*params.values()):\n",
    "            runs.append(Run(*value))\n",
    "            \n",
    "        return runs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "        lr = [0.01, 0.001],\n",
    "        batch_size = [50, 100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runbuilder.get_runs(params):\n",
    "    comment = f'-{run}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229852c5",
   "metadata": {},
   "source": [
    "__Explanation of above mentioned run builder class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b9ca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded022b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = namedtuple('Run', params.keys()) #namedtuple is a factory function for making a tuple class. With that class we can create tuples that are callable by name also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0754f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "for values in product(*params.values()):  # * indicate to accept tuple values as arguments oppose to tuple itself.\n",
    "    runs.append(run(*values))\n",
    "runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f65488",
   "metadata": {},
   "source": [
    "__Explanation ends__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b179f14",
   "metadata": {},
   "source": [
    "__Application of runbuilder class on training model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "        lr = [0.01, 0.001],\n",
    "        batch_size = [50, 100]\n",
    ")\n",
    "\n",
    "\n",
    "for run in runbuilder.get_runs(params):\n",
    "    comment = f'-{run}'\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = shuffle, num_workers = 1, pin_memory = True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr = lr, momentum = momentum)\n",
    "\n",
    "    model.train()\n",
    "    print(f'This model is being trained on {device}.')\n",
    "\n",
    "    images, labels = next(iter(train_loader))\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "\n",
    "    #comment = f' batch_size = {batch_size}, lr = {lr}'\n",
    "\n",
    "    # adding above mentioned comment to summary writer will uniquely identify runs inside tensor board because summarywriter will append the comment to the name of the run.\n",
    "\n",
    "    tb = SummaryWriter(comment=comment)\n",
    "    tb.add_image('images', grid)\n",
    "    tb.add_graph(model.to(device), images.to(device))\n",
    "\n",
    "\n",
    "    for epoch in range(2):\n",
    "\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch_no, (images, label) in enumerate(train_loader):\n",
    "            images, label = images.to(device), label.to(device) # .to(device) = .to('cuda') or .to('cpu') whichever is available\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)   # Pass batch\n",
    "            loss = criterion(output, label) # calculate loss\n",
    "            loss.backward()  # calculate gradients\n",
    "            optimizer.step() # update weights\n",
    "\n",
    "            total_loss += loss.item() * batch_size # we are multiplying with batch size to account for different batchsizes  \n",
    "            total_correct += get_num_correct(output, label)\n",
    "\n",
    "            if batch_no % 10 == 0:\n",
    "                print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                        epoch, batch_no * len(images), len(train_loader.dataset),\n",
    "                        100. * batch_no / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "        tb.add_scalar('Loss', total_loss, epoch)\n",
    "        tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "        tb.add_scalar('Accuracy', total_correct/len(train_ds), epoch)\n",
    "\n",
    "    #     tb.add_histogram('conv1.bias', model.conv1.bias, epoch)\n",
    "    #     tb.add_histogram('conv1.weight', model.conv1.weight, epoch)\n",
    "    #     tb.add_histogram('conv1.weight.grad', model.conv1.weight.grad, epoch)\n",
    "\n",
    "        for name, weight in model.named_parameters():\n",
    "            tb.add_histogram(tag = name, values = weight, global_step = epoch)\n",
    "            tb.add_histogram(f'{name}.grad:   ', weight.grad, epoch)\n",
    "\n",
    "        print(\"epoch\", epoch,\n",
    "              \"total_correct\", total_correct,\n",
    "              \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bac32",
   "metadata": {},
   "source": [
    "### Testing model\n",
    "\n",
    "The model.train() sets the modules in the network in training mode.\n",
    "It tells our model that we are currently in the training phase \n",
    "so the model keeps some layers, like dropout, batch-normalization \n",
    "which behaves differently depends on the current phase, active.\n",
    "\n",
    "Whereas the model.eval() does the opposite.\n",
    "Therefore, once the model.eval() has been called then, our model deactivate such layers \n",
    "so that the model outputs its inference as is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb9901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_predictions(model, loader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_predictions = torch.tensor([]).to(device)\n",
    "        #print(f' first slot {all_predictions}')\n",
    "    \n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model.forward(images)       # This will return two outputs, because in our final linear layer,   \n",
    "                                                 # output value is 2. These are 2 probabilites for each class.\n",
    "            #print(f'second slot {all_predictions}')\n",
    "            #print(f'output_1 {output}')\n",
    "                \n",
    "            all_predictions = torch.cat((all_predictions, output), dim=0) # Concatenates the given sequence of :attr:`seq` tensors in the given dimension.\n",
    "                                           # All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
    "            \n",
    "            #print(f'third slot {all_predictions}')\n",
    "            #print(f'output_2 {output}')\n",
    "              \n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b79a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = get_all_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5345262a",
   "metadata": {},
   "source": [
    "### Checking correct and accurate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = get_num_correct(test_predictions, torch.FloatTensor(test_dataset.targets).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda897c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total correct:', correct_predictions)\n",
    "print('accuracy:', correct_predictions/len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b4f20",
   "metadata": {},
   "source": [
    "### Building confusion matrix and classification report\n",
    "\n",
    "The task in building the confusion matrix is to count the number of predicted values against the true values (targets). This will create a matrix that acts as a heat map telling us where the predicted values fall relative to the true values. \n",
    "\n",
    "__To do this, we need to have the targets tensor and the predicted label from the train_preds tensor.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146c5f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_predictions.argmax(dim=1))  # argmax returns the indices of the maximum values of a tensor across a dimension.\n",
    "#print(test_dataset.targets)\n",
    "print(type(test_predictions.argmax(dim=1)))\n",
    "print(test_predictions.argmax(dim=1).shape)\n",
    "print(type(test_dataset.targets))\n",
    "print(len(test_dataset.targets))\n",
    "print(len(test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(test_dataset.targets, test_predictions.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "classification_report = classification_report(test_dataset.targets, test_predictions.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67377176",
   "metadata": {},
   "source": [
    "__Drawing confusion matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4571b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_confusion_matrix(conf_mat=confusion_matrix, colorbar = True, show_normed= True, figsize=(6, 6), class_names=classes, cmap=plt.cm.Blues)\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "# plt.xticks(tick_marks, classes, rotation=45)\n",
    "# plt.yticks(tick_marks, classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1621a",
   "metadata": {},
   "source": [
    "### Saving or Loading model\n",
    "\n",
    "When it comes to saving and loading models, there are three core functions to be familiar with:\n",
    "\n",
    "1. torch.save: Saves a serialized object to disk. This function uses Python’s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "\n",
    "2. torch.load: Uses pickle’s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into (see Saving & Loading Model Across Devices).\n",
    "\n",
    "3. torch.nn.Module.load_state_dict: Loads a model’s parameter dictionary using a deserialized state_dict. For more information on state_dict.\n",
    "\n",
    "__What is a state_dict?__\n",
    "\n",
    "In PyTorch, the learnable parameters (i.e. weights and biases) of a torch.nn.Module model are contained in the model’s parameters (accessed with model.parameters()). A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor. Note that only layers with learnable parameters (convolutional layers, linear layers, etc.) and registered buffers (batchnorm’s running_mean) have entries in the model’s state_dict. Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizer’s state, as well as the hyperparameters used.\n",
    "\n",
    "__Because state_dict objects are Python dictionaries, they can be easily saved, updated, altered, and restored, adding a great deal of modularity to PyTorch models and optimizers.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9af9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4738a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb862e3",
   "metadata": {},
   "source": [
    "__Save/Load state_dict (Recommended)__\n",
    "\n",
    "__Save:__   torch.save(model.state_dict(), PATH)\n",
    "\n",
    "__Load:__   model = TheModelClass(*args, **kwargs)<br>     model.load_state_dict(torch.load(PATH))<br>\n",
    "            model.eval()\n",
    "\n",
    "\n",
    "A common PyTorch convention is to save models using either a .pt or .pth file extension.\n",
    "\n",
    "Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    "\n",
    "\n",
    "__Save/Load Entire Model__\n",
    "\n",
    "__Save:__ torch.save(model, PATH)\n",
    "\n",
    "__Load:__ Model class must be defined somewhere <br>\n",
    "model = torch.load(PATH) <br>\n",
    "model.eval()\n",
    "\n",
    "__Saving & Loading a General Checkpoint for Inference and/or Resuming Training__\n",
    "\n",
    "__Save:__\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            ...\n",
    "            }, PATH)\n",
    "\n",
    "__Load:__\n",
    "\n",
    "model = TheModelClass(*args, **kwargs) <br>\n",
    "optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval() - or - model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d959fee",
   "metadata": {},
   "source": [
    "### Saving and Loading model state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model state_dict\n",
    "\n",
    "torch.save(model.state_dict(), '/home/a/Documents/Pytorch/CNNs/cats_dogs/model_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model state_dict\n",
    "\n",
    "model = cnn()\n",
    "model.load_state_dict(torch.load('/home/a/Documents/Pytorch/CNNs/cats_dogs/model_state_dict.pt'))\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39687fa0",
   "metadata": {},
   "source": [
    "### Saving and Loading complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c536ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model state_dict\n",
    "\n",
    "torch.save(model, '/home/a/Documents/Pytorch/CNNs/cats_dogs/complete_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model state_dict\n",
    "\n",
    "model = cnn()\n",
    "model = torch.load('/home/a/Documents/Pytorch/CNNs/cats_dogs/complete_model.pt')\n",
    "model.eval()"
   ]
  },
 ,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e3a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
